I didn't compare the performance of these two algorithms, but the "list-then-eliminate" algorithm definitely consumes more memory. In this small input space example, I used at least 65536 bytes for recording the result of the calculation. On the other hand, I used very few memory in partA. Another thing I found is that looks like if the test case didn't show up before, then the vote will be always 50-50. But if it appeared before, then the voting result will be totally the same. I think this is obvious since one input vector could be either 2 results, "high" or "low". Therefore, if we don't know the result from training data, there should be an equal possibility for the result of this input vector. However, this means that if we use this algorithm and the data never shown before, then it's no use to vote among our version space.
Another finding is that every input vector could eliminate half size of the version space. Therefore, we only need 16 different inputs and then we can converge to only one hypothesis.
