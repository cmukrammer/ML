The prediction of our decision tree is more accurate in both training data and testing data for politician dataset. I think this is because, in education dataset, the best attribute we can pick maxmize the mutual information is M2, but in the case of "M2 = A", the number of '+' and '-' case are too close (13+/15-), which introduces a high uncertainty to this decision. On the other hand, in politician dataset, we always find a fairly good attribute to minimize the uncertainty. This is what I observed from these two dataset.
